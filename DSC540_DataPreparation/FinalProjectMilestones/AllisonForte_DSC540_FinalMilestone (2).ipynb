{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcada0de",
   "metadata": {},
   "source": [
    "Allison Forte\n",
    "\n",
    "DSC 540\n",
    "\n",
    "Project: Final Milestone (5)\n",
    "\n",
    "August 13, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3507ec",
   "metadata": {},
   "source": [
    "# Storing the data in a database, merging it, and visualizing it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98e9a5",
   "metadata": {},
   "source": [
    "Load your 3 cleaned and transformed datasets into a database. \n",
    "Load each dataset into SQL Lite as an individual table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af7042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages and establish a connection and the cursor\n",
    "\n",
    "import sqlite3\n",
    "con = sqlite3.connect('my_dsc540_database')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b491f",
   "metadata": {},
   "source": [
    "Create table with CSV file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64fec569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allison.forte/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:2872: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  sql.to_sql(\n"
     ]
    }
   ],
   "source": [
    "# pull in cleaned data from CSV from milestone 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def monthToNum(shortMonth):\n",
    "    return {\n",
    "            'Jan': 1,\n",
    "            'Feb': 2,\n",
    "            'Mar': 3,\n",
    "            'Apr': 4,\n",
    "            'May': 5,\n",
    "            'Jun': 6,\n",
    "            'Jul': 7,\n",
    "            'Aug': 8,\n",
    "            'Sep': 9, \n",
    "            'Oct': 10,\n",
    "            'Nov': 11,\n",
    "            'Dec': 12}[shortMonth]\n",
    "\n",
    "\n",
    "file = pd.read_csv('/Users/allison.forte/Documents/540 Assignments/tech_fundings.csv')\n",
    "df = pd.DataFrame(data=file)  # Create df\n",
    "df.rename(columns = {'Region':'Country', 'Vertical':'Category', 'Funding Amount (USD)':'Funding_amt', \n",
    "                     'Funding Date':'Funding_date'}, inplace = True)  # Rename columns\n",
    "df = df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)  # Drop missing data\n",
    "df['Funding_amt'] = df['Funding_amt'].replace(['Unknown'],'0')  # Fix missing data\n",
    "df = df.drop_duplicates(subset='Website', keep='first', inplace=False)  # Drop duplicates\n",
    "df[['Funding_month', 'Funding_year']] = df['Funding_date'].str.split('-', expand=True)  # Split date to 2 columns\n",
    "months = (df['Funding_month'])\n",
    "month_num  = []\n",
    "for m in months:\n",
    "    month_num.append(monthToNum(m))\n",
    "df['month_num'] = month_num  # Add month number to column\n",
    "df['Funding_amt'] = df['Funding_amt'].astype(float)  # Change data type\n",
    "df['Funding_year'] = df['Funding_year'].astype(float)  # Change data type\n",
    "df = df[(df['Funding_amt'] <70000000) & (df['Funding_amt'] > 0)]  # Remove outliers\n",
    "df['Country'] = df['Country'].str.lower()\n",
    "\n",
    "\n",
    "# insert df into my_dsc540_database\n",
    "\n",
    "df.to_sql(name = 'csv_data', con = con, if_exists = 'replace', index = False )\n",
    "\n",
    "\n",
    "# commit the changes\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17eccf",
   "metadata": {},
   "source": [
    "Create table with the website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba55500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in cleaned data from website from milestone 3\n",
    "\n",
    "import lxml\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url1 = 'https://www.iban.com/exchange-rates'\n",
    "dfs1 = pd.read_html(url1)\n",
    "df_convert = dfs1[0]\n",
    "\n",
    "# Add one row with EUR conversion rate since the table is based on a EUR conversion rate\n",
    "new_row = {'Currency':'EUR', 'Currency Name': 'Euro', 'Exchange Rate = 1 EUR':1, 'Convert':'NaN'}\n",
    "df_convert = df_convert.append(new_row, ignore_index=True)\n",
    "\n",
    "# Second source needed for the exchange rates- original data contained 'number' that was not the exchange rate\n",
    "url2 = 'https://www.iban.com/currency-codes'\n",
    "dfs2 = pd.read_html(url2)\n",
    "df_codes = dfs2[0]\n",
    "\n",
    "df_convert.rename(columns = {'Currency':'Currency_code', 'Currency Name':'Currency_name', \n",
    "                             'Exchange Rate = 1 EUR':'Exchange_rate_1EUR'}, inplace = True) # Change headers in df_convert\n",
    "df_codes.rename(columns = {'Currency':'Currency_name', 'Code':'Currency_code'}, inplace = True) # Change headers in df_codes\n",
    "df_convert.drop(columns = ['Convert'], inplace = True)  # Drop unneeded rows from df_convert\n",
    "df_codes.drop(columns=['Number'], inplace = True)  # Drop unneeded rows from df_codes\n",
    "df_codes.dropna(axis = 0, how = 'any', thresh = None, subset = None, inplace = True)  # Fixing missing data in df_codes\n",
    "final_df = df_codes.merge(df_convert, on = 'Currency_code', suffixes=('', '_drop'))  # Merge the 2 data sets\n",
    "final_df.drop(columns=['Currency_name_drop'], inplace = True)  # Drop the second currency_name column\n",
    "final_df['Country'] = final_df['Country'].str.lower()\n",
    "final_df\n",
    "\n",
    "\n",
    "# insert final_df into my_dsc540_database\n",
    "\n",
    "final_df.to_sql(name = 'website_data', con = con, if_exists = 'replace', index = False )\n",
    "\n",
    "\n",
    "# commit the changes\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059519f",
   "metadata": {},
   "source": [
    "Create table with the API data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018d0dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for \"EUR\".\n",
      "Retrieving data for \"USD\".\n",
      "Retrieving data for \"AUD\".\n",
      "Retrieving data for \"INR\".\n",
      "Retrieving data for \"NOK\".\n",
      "Retrieving data for \"BRL\".\n",
      "Retrieving data for \"BGN\".\n",
      "Retrieving data for \"CAD\".\n",
      "Retrieving data for \"CNY\".\n",
      "Retrieving data for \"NZD\".\n",
      "Retrieving data for \"HRK\".\n",
      "Retrieving data for \"CZK\".\n",
      "Retrieving data for \"DKK\".\n",
      "Retrieving data for \"GBP\".\n",
      "Retrieving data for \"HKD\".\n",
      "Retrieving data for \"HUF\".\n",
      "Retrieving data for \"ISK\".\n",
      "Retrieving data for \"IDR\".\n",
      "Retrieving data for \"ILS\".\n",
      "Retrieving data for \"JPY\".\n",
      "Retrieving data for \"KRW\".\n",
      "Retrieving data for \"ZAR\".\n",
      "Retrieving data for \"CHF\".\n",
      "Retrieving data for \"MYR\".\n",
      "Retrieving data for \"MXN\".\n",
      "Retrieving data for \"PHP\".\n",
      "Retrieving data for \"PLN\".\n",
      "Retrieving data for \"RON\".\n",
      "Retrieving data for \"SGD\".\n",
      "Retrieving data for \"SEK\".\n",
      "Retrieving data for \"THB\".\n",
      "Retrieving data for \"TRY\".\n"
     ]
    }
   ],
   "source": [
    "# pull in cleaned data from API from milestone 4\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "with open('/Users/allison.forte/Downloads/API keys') as f:  # Set up connection to API using API key\n",
    "\n",
    "    api_keys = f.readlines()\n",
    "    api_key = api_keys[9]\n",
    "\n",
    "\n",
    "def search_currency(currency):\n",
    "    try:\n",
    "        code = currency\n",
    "        url = 'https://v6.exchangerate-api.com/v6/' + api_key + 'latest/' + code\n",
    "        print(f'Retrieving data for \"{currency}\".')  # Let the user know the search is working\n",
    "        response = requests.get(url)  # Accessing the API\n",
    "        data = response.json()  # Data received from the API\n",
    "\n",
    "        if data['result'] == 'success':\n",
    "            return(data)\n",
    "    \n",
    "        else:\n",
    "            print(\"Error encountered\")  # Informs user of error\n",
    "    \n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"ERROR: {e.reason}\")  # Displays error if the request is not successful\n",
    "\n",
    "\n",
    "code = final_df['Currency_code']  #  Pull the currency codes only and remove duplicates\n",
    "codes = code.drop_duplicates(keep='first', inplace=False)\n",
    "\n",
    "responses = [] \n",
    "\n",
    "for c in codes:\n",
    "    responses.append(search_currency(c))\n",
    "        \n",
    "rates = {}\n",
    "p = 0\n",
    "\n",
    "for r in responses:\n",
    "    rates[p] = r['conversion_rates']\n",
    "    p=p+1\n",
    "        \n",
    "rates_df = pd.DataFrame.from_dict(rates, orient = 'columns')\n",
    "\n",
    "names = {0: 'EUR', 1: 'USD', 2: 'AUD', 3:'INR', 4:'NOK', 5:'BRL', 6: 'BGN', 7: 'CAD', 8: 'CNY',\n",
    "         9: 'NZD', 10: 'HRK', 11: 'CZK', 12: 'DKK', 13: 'GBP', 14: 'HKD', 15: 'HUF', 16: 'ISK',\n",
    "         17: 'IDR', 18: 'ILS', 19: 'JPY', 20: 'KRW', 21: 'ZAR', 22: 'CHF', 23: 'MYR', 24: 'MXN', \n",
    "         25: 'PHP', 26: 'PLN', 27: 'RON', 28: 'SGD', 29: 'SEK', 30: 'THB', 31: 'TRY'}\n",
    "rates_df.rename(columns=names, inplace = True)\n",
    "\n",
    "\n",
    "# insert rates_df into my_dsc540_database\n",
    "\n",
    "rates_df.to_sql(name = 'api_data', con = con, if_exists = 'replace', index = True )\n",
    "\n",
    "\n",
    "# commit the changes\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710d923",
   "metadata": {},
   "source": [
    "Join the datasets together in Python into 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6defa0cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = cur.execute(\"SELECT * FROM csv_data as csv \\\n",
    "                     LEFT JOIN website_data as web ON csv.Country = web.Country\\\n",
    "                     LEFT JOIN api_data as api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbfee7",
   "metadata": {},
   "source": [
    "Once all the data is merged together in your database, create 5 visualizations that demonstrate the data you have cleansed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031df915",
   "metadata": {},
   "source": [
    "# Write a 250-500-word summary of what you learned and had to do to complete the project. \n",
    "\n",
    "In your write-up, address the ethical implications of cleansing data and your project topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d0e0b",
   "metadata": {},
   "source": [
    "This project exposed me to processes that I was not familiar with. I had never pulled data from a website, and I had not used an API in this way. I appreciated the opportunity to explore these data sources and how to access them, clean them, and think through combining them with other data sources.  \n",
    "\n",
    "I still have a lot to learn with APIs especially. I am getting more comfortable working with JSON data, but it still takes a lot to parse it and create something usable.  \n",
    "\n",
    "For websites, I now know how to look at the source code and understand a bit more about the components of the website. This helps to understand exactly what I am trying to pull. While I will still need practice with this method of data extraction, the experience in this course seemed like a solid introduction to how most websites can be scrapped. \n",
    "\n",
    "I am most comfortable working with CSVs because I have the most experience with them. Starting with the CSV was helpful for me because I was able to look at the data and think about how I wanted to clean it which gave me ideas for cleaning the website and API data.  \n",
    "\n",
    "One thing I was not prepared for was loading my data in SQLLite and then joining it. Doing a project like this in the future I would think about an entity relationship diagram early on so I could be sure my data would be easily matched. Learning about this in the last 2 weeks was extremely helpful and will allow me to be better prepared for projects in future classes.  \n",
    "\n",
    "While I was unable to add visuals to this write-up, I do have some ethical concerns about where this project was heading. Looking at funding levels for companies in various countries could impact where investors are willing to put their money. If it were discovered that companies in specific countries tend to fund faster, perhaps investors would focus heavily on those countries while avoiding others. This data could also be combined with the first year's profits for a potentially problematic result. On the other hand, if the data could be used to discover which countries are able to make the most profits with the least funding capital, perhaps it would be worth further investigation into why that is to improve results in other countries.  \n",
    "\n",
    "Data regarding finances across the world always has issues of equivalency. While this data was compiled to include exchange rates which should account for that, living conditions, lifestyle, and average income will always have an impact beyond what we can account for with a straightforward exchange rate. While this data could be a starting point, a lot more exploration could happen after an initial analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
